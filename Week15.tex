
\documentclass{article}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2552}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{Created=Tuesday, December 22, 2015 20:16:57}
%TCIDATA{LastRevised=Tuesday, December 22, 2015 23:14:35}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Scientific Notebook\Blank Document">}
%TCIDATA{CSTFile=Math with theorems suppressed.cst}
%TCIDATA{PageSetup=72,72,72,72,0}
%TCIDATA{AllPages=
%F=36,\PARA{038<p type="texpara" tag="Body Text" >\hfill \thepage}
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{tcilatex}

\begin{document}


\bigskip 8.1 Choose $\hat{\theta}_{n}\left( \vec{X}\right) =\frac{3\underset{%
i=1}{\overset{n}{\Sigma }}X_{i}}{n}\implies E\left( \hat{\theta}_{n}\left( 
\vec{X}\right) \right) =3E\left( X_{1}\right) $

$=3\int_{-1}^{1}\frac{1}{2}x\left( 1+\theta x\right) dx=\theta \implies \hat{%
\theta}_{n}\left( \vec{X}\right) $ is unbiased.

Also $D\left( X_{1}\right) $ is finite $\implies D\left( \hat{\theta}%
_{n}\left( \vec{X}\right) \right) =\frac{3D\left( X_{1}\right) }{n}%
\rightarrow 0.$

By Chebshev's inequality, $P\left( \left\vert \hat{\theta}_{n}\left( \vec{X}%
\right) -\theta \right\vert >\epsilon \right) <\frac{D\left( \hat{\theta}%
_{n}\left( \vec{X}\right) \right) }{\epsilon ^{2}}\rightarrow 0$

$\implies \hat{\theta}_{n}\left( \vec{X}\right) $ is a consistent sequence
of estimator of parameter $\theta .$

8.2 $\forall \delta >0$

$P_{\theta }\left( \left\vert a_{n}W_{n}+b_{n}-\theta \right\vert >\delta
\right) $

$=$ $P_{\theta }\left( \left\vert W_{n}-\theta +\left( a_{n}-1\right)
W_{n}+b_{n}\right\vert >\delta \right) $

Since $W_{n}$ is a consistence sequence of estimator of parameter $\theta ,$

$P_{\theta }\left( \left\vert W_{n}-\theta \right\vert >\delta \right)
\rightarrow 0,$ $\implies P_{\theta }\left( \left\vert W_{n}\right\vert
>\left\vert \theta \right\vert +\delta \right) <$

$P_{\theta }\left( \left\vert W_{n}-\theta \right\vert >\delta \right)
\rightarrow 0$ $\implies W_{n}$ is bounded with respect to probability $%
P_{\theta }$.

$\leq P_{\theta }\left( \left\vert W_{n}-\theta +\left( a_{n}-1\right)
W_{n}+b_{n}\right\vert >\delta ,\left\vert W_{n}\right\vert <M\right)
+P\left( \left\vert W_{n}\right\vert \geq M\right) $

\bigskip $\leq +P(\left\vert W_{n}\right\vert \geq M)$

and $a_{n}\rightarrow 1,b_{n}\rightarrow 0$

$\implies \forall \epsilon ,\exists N,s.tP(\left\vert W_{n}\right\vert \geq
M)<\frac{\epsilon }{2},P_{\theta }\left( \left\vert W_{n}-\theta \right\vert
>\frac{\delta }{2}\right) <\frac{\epsilon }{2}$ and $\delta -\left(
\left\vert a_{n}-1\right\vert M+\left\vert b_{n}\right\vert \right) >\frac{%
\delta }{2},\forall n>N,$

$\implies P_{\theta }\left( \left\vert W_{n}-\theta \right\vert >\delta
-\left( \left\vert a_{n}-1\right\vert M+\left\vert b_{n}\right\vert \right)
\right) <$

$P_{\theta }\left( \left\vert W_{n}-\theta \right\vert >\frac{\delta }{2}%
\right) <\frac{\epsilon }{2}\implies P_{\theta }\left( \left\vert
a_{n}W_{n}+b_{n}-\theta \right\vert >\delta \right) <\epsilon .$

$\implies a_{n}W_{n}+b_{n}-\theta \overset{P}{\rightarrow }0.$

Therefore $U_{n}=a_{n}W_{n}+b_{n}$ is a consistent sequence of estimator of 

parameter $\theta .$

10.3 $\left( 1\right) \log L\left( \theta |x_{1},..x_{n}\right) =\underset{%
i=1}{\overset{n}{\sum }}\log \frac{1}{\sqrt{2\pi \theta }}e^{-\frac{\left(
x_{i}-\theta \right) ^{2}}{2\theta }}.$

$\frac{d}{d\theta }\log L=0\implies -\underset{i=1}{\overset{n}{\sum }}\frac{%
x_{i}^{2}}{\theta ^{2}}+n+\frac{n}{\theta }=0\implies $

$\theta ^{2}+\theta -\frac{\underset{i=1}{\overset{n}{\sum }}x_{i}^{2}}{n}=0.
$That is $\hat{\theta}_{n}^{2}+\hat{\theta}_{n}-\frac{\underset{i=1}{\overset%
{n}{\sum }}x_{i}^{2}}{n}=0$

By Vi\'{e}te's Thm, we know that the two roots have different signs.

The positive root is MLE.

$\left( 2\right) $From $\left( 10.1.7\right) $ $Var_{\theta }\left( \hat{%
\theta}_{n}\right) =\frac{1}{-\frac{\partial ^{2}}{\partial \theta ^{2}}\log
L\left( \theta |\vec{x}\right) _{|\theta =\hat{\theta}_{n}}}=\frac{1}{%
\underset{i=1}{\overset{n}{\sum }}\frac{2x_{i}^{2}}{\theta ^{3}}-\frac{n}{%
\theta ^{2}}}_{|\theta =\hat{\theta}}$

$=\frac{1}{\underset{i=1}{\overset{n}{\sum }}\frac{2x_{i}^{2}}{\hat{\theta}%
_{n}^{3}}-\frac{n}{\hat{\theta}_{n}^{2}}},$from $\hat{\theta}_{n}^{2}+\hat{%
\theta}_{n}-\frac{\underset{i=1}{\overset{n}{\sum }}x_{i}^{2}}{n}=0$ follows 
$Var_{\theta }\left( \hat{\theta}_{n}\right) $

$=\frac{\hat{\theta}_{n}^{2}}{n\left( 2\hat{\theta}_{n}+1\right) }\implies
Var\sqrt{n}\left( \hat{\theta}_{n}-\theta \right) =\frac{\hat{\theta}_{n}^{2}%
}{\left( 2\hat{\theta}_{n}+1\right) }\rightarrow \frac{\theta ^{2}}{2\theta
+1}\implies $

$\sqrt{n}\left( \hat{\theta}_{n}-\theta \right) \overset{d}{\rightarrow }%
N\left( 0,\frac{\theta ^{2}}{2\theta +1}\right) .$

10.4 $\left( 1\right) \frac{\underset{i=1}{\overset{n}{\sum }}X_{i}Y_{i}}{%
\underset{i=1}{\overset{n}{\sum }}X_{i}^{2}}=\frac{\underset{i=1}{\overset{n}%
{\sum }}X_{i}\left( \beta X_{i}+\epsilon _{i}\right) }{\underset{i=1}{%
\overset{n}{\sum }}X_{i}^{2}}=\beta +\frac{\underset{i=1}{\overset{n}{\sum }}%
X_{i}\epsilon _{i}}{\underset{i=1}{\overset{n}{\sum }}X_{i}^{2}},$

$E\left( above\right) =\beta +E\frac{\underset{i=1}{\overset{n}{\sum }}X_{i}%
}{\underset{i=1}{\overset{n}{\sum }}X_{i}^{2}}E\epsilon _{i}=\beta ,$ from
the indepedent property of $X_{s}$ and $\epsilon _{s}.$

$D\left( above\right) =E\left( \frac{\underset{i=1}{\overset{n}{\sum }}%
X_{i}\epsilon _{i}}{\underset{i=1}{\overset{n}{\sum }}X_{i}^{2}}\right)
^{2}=E\left( \frac{\underset{i=1}{\overset{n}{\sum }}X_{i}^{2}\epsilon
_{i}^{2}}{\left( \underset{i=1}{\overset{n}{\sum }}X_{i}^{2}\right) ^{2}}%
\right) \approx \frac{\underset{i=1}{\overset{n}{\sum }}EX_{i}^{2}E\epsilon
_{i}^{2}}{\left( \underset{i=1}{\overset{n}{\sum }}EX_{i}^{2}\right) ^{2}}=%
\frac{\sigma ^{2}}{n\left( \tau ^{2}+\mu ^{2}\right) }.$

$\left( 2\right) \frac{\underset{i=1}{\overset{n}{\sum }}Y_{i}}{\underset{i=1%
}{\overset{n}{\sum }}X_{i}}=\frac{\underset{i=1}{\overset{n}{\sum }}\beta
X_{i}+\epsilon _{i}}{\underset{i=1}{\overset{n}{\sum }}X_{i}}=\beta +\frac{%
\underset{i=1}{\overset{n}{\sum }}\epsilon _{i}}{\underset{i=1}{\overset{n}{%
\sum }}X_{i}}\implies E\frac{\underset{i=1}{\overset{n}{\sum }}Y_{i}}{%
\underset{i=1}{\overset{n}{\sum }}X_{i}}=\beta $

$D\left( above\right) =E\left( \frac{\underset{i=1}{\overset{n}{\sum }}%
\epsilon _{i}}{\underset{i=1}{\overset{n}{\sum }}X_{i}}\right) ^{2}=E\left( 
\underset{i=1}{\overset{n}{\sum }}\epsilon _{i}\right) ^{2}E\frac{1}{\left( 
\underset{i=1}{\overset{n}{\sum }}X_{i}\right) ^{2}},$

$\underset{i=1}{\overset{n}{\sum }}\epsilon _{i}\symbol{126}N\left(
0,n\sigma ^{2}\right) \implies E\left( \underset{i=1}{\overset{n}{\sum }}%
\epsilon _{i}\right) ^{2}=n\sigma ^{2}$

Let $X=\underset{i=1}{\overset{n}{\sum }}X_{i}\symbol{126}N\left( n\mu
,n\tau ^{2}\right) ,$ by Delta's Method, $E\left( \frac{1}{X^{2}}\right)
\approx \frac{1}{EX^{2}}=\frac{1}{n^{2}\mu ^{2}+n^{2}\tau ^{2}}$

$\implies D\frac{\underset{i=1}{\overset{n}{\sum }}\epsilon _{i}}{\underset{%
i=1}{\overset{n}{\sum }}X_{i}}=\frac{\sigma ^{2}}{n\mu ^{2}+n\tau ^{2}}.$

$\left( 3\right) \frac{1}{n}\underset{i=1}{\overset{n}{\sum }}\frac{Y_{i}}{%
X_{i}}=\frac{1}{n}\underset{i=1}{\overset{n}{\sum }}\left( \beta +\frac{%
\epsilon _{i}}{X_{i}}\right) =\beta +\underset{i=1}{\overset{n}{\sum }}\frac{%
\epsilon _{i}}{X_{i}}\implies $

$E\left( above\right) =\beta ,$

$D\left( above\right) =\frac{1}{n^{2}}E\left( \underset{i=1}{\overset{n}{%
\sum }}\frac{\epsilon _{i}}{X_{i}}\right) ^{2}=\frac{1}{n}E\frac{\epsilon
_{1}^{2}}{X_{1}^{2}}\approx \frac{1}{n}\frac{E\epsilon _{1}^{2}}{EX_{1}^{2}}=%
\frac{1}{n}\frac{\sigma ^{2}}{\tau ^{2}+\mu ^{2}},$

Conclusion, the statistics of $\left( 1\right) \symbol{126}\left( 3\right) $
have the same expectation $\beta $ and approximated variance  $\frac{1}{n}%
\frac{\sigma ^{2}}{\tau ^{2}+\mu ^{2}}.$

\end{document}
